{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "For any issues please refer refer to the bibliography in the README or reach me out by email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ac252489\\AppData\\Local\\Temp\\ipykernel_15712\\2000416229.py:2: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# full width notebook\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Required packages to run the script\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticke\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "import math\n",
    "import os\n",
    "import pickle as pkl\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import lsqr\n",
    "import scipy.fftpack\n",
    "import xlsxwriter\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Importing functions \n",
    "from import_excel import import_excel\n",
    "from calc_translations import load_trans\n",
    "from calc_translations import translate\n",
    "from wavelet_decomposition import compute_betas\n",
    "from wavelet_decomposition import stack_betas\n",
    "from wavelet_decomposition import preplotprocessing\n",
    "from wavelet_decomposition import reconstruct\n",
    "from wavelet_decomposition import sine_function\n",
    "from plots import plot_betas_heatmap\n",
    "from plots import plot_EPN\n",
    "from calc_EPN import calc_epn\n",
    "from plots import fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All control parameters goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Path\n",
    "path_input_data = \"\"\n",
    "input_file = 'input_time_series.xlsx'\n",
    "#\n",
    "beta_path = \"betas\\\\square_shape\\\\\" # Results of the wavelet decomposition are saved here in .xlsx files\n",
    "\n",
    "#\n",
    "path_trans = 'translation//' # Translations are saved here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Options of the wavelet decomposition\n",
    "vy = 6  # vectors per yearsave\n",
    "vw = 3  # vectors per week\n",
    "vd = 6  # vectors per day\n",
    "#\n",
    "# Time scales of the wavelet decomposition\n",
    "time_scales = [0.75, 1.5, 3., 6., 12, 24., 42., 84., 168., 273.75, 547.5, 1095., 2190., 4380.,\n",
    "              8760.] # cycles length, in hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing excel time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Time series are imported from excel files. The should be formated like the example \"input_time_series.xlsx\".\n",
    "     * Time Series are provided by trhe French TSO, RTE (https://www.rte-france.com/fr/eco2mix/eco2mix)\n",
    "     * input_time_series.xlsx provides 7 years of French Wind power, Solar PV production and consumption\n",
    " \n",
    " <br />\n",
    " \n",
    "* All years are renormalized to 1 MW so that They can be compared with one another\n",
    "    * Signal is the renormalized by load_factor = 54 GW  for the French case\n",
    "    \n",
    "<br />\n",
    "\n",
    "* For mathematical purposes signals need to have 64 data per day. The functioninport_excel() interpolate signals from dpd points to ndpd = 64\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Controle parameters -------\n",
    "\n",
    "# Wich time series do you want to import from the excel file ?\n",
    "time_series = ['Consumption', 'Wind', 'PV']\n",
    "# for the signal interpolation\n",
    "dpd = 48 # data per year\n",
    "ndpd = 64 # new data per year (for the interpolation)\n",
    "dpy = 365 # data per year :  cut the leap years to 365 years (annee bissexitle)\n",
    "signal_length = ndpd * dpy\n",
    "\n",
    "#\n",
    "load_factor = 54e+3 # MW\n",
    "#Mean value of electricity load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 7 years imported\n",
      "['2012', '2013', '2014', '2015', '2016', '2017', '2018']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not iterable (sheet: 2012)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# ----------------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# ------ Importing data ------------\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# ----------------------------------\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m stacked_input_data, years \u001b[39m=\u001b[39m import_excel(path_input_data,input_file, \n\u001b[0;32m      6\u001b[0m                                     dpd ,ndpd, dpy, time_series, \n\u001b[0;32m      7\u001b[0m                                     interp\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39m# interpolate data from dpd to ndpd numper of points per day\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# ------ Saving data -------\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# Once data are computed, comment lines bellow and uncomment the last one\u001b[39;00m\n\u001b[0;32m     11\u001b[0m pkl\u001b[39m.\u001b[39mdump([stacked_input_data, years], \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mimported_signal\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.p\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) )\n",
      "File \u001b[1;32mc:\\Users\\ac252489\\wavelet_decomposition\\import_excel.py:33\u001b[0m, in \u001b[0;36mimport_excel\u001b[1;34m(path_input_data, input_file, dpd, ndpd, dpy, time_series, interp)\u001b[0m\n\u001b[0;32m     31\u001b[0m tmp_data \u001b[39m=\u001b[39m []\n\u001b[0;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m yr \u001b[39min\u001b[39;00m years: \u001b[39m# loop over the sheets of the excel file\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_excel(path_input_data \u001b[39m+\u001b[39;49m input_file,\n\u001b[0;32m     34\u001b[0m                         sheet_name \u001b[39m=\u001b[39;49m yr,\n\u001b[0;32m     35\u001b[0m                         names \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     36\u001b[0m                        skiprows \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39m# skip firs row\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[39m# Cut leap year\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     one_yr \u001b[39m=\u001b[39m df[energy_ts]\u001b[39m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\ac252489\\wavelet_decomposition\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:508\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEngine should not be specified when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n\u001b[0;32m    507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 508\u001b[0m     data \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mparse(\n\u001b[0;32m    509\u001b[0m         sheet_name\u001b[39m=\u001b[39;49msheet_name,\n\u001b[0;32m    510\u001b[0m         header\u001b[39m=\u001b[39;49mheader,\n\u001b[0;32m    511\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    512\u001b[0m         index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[0;32m    513\u001b[0m         usecols\u001b[39m=\u001b[39;49musecols,\n\u001b[0;32m    514\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    515\u001b[0m         converters\u001b[39m=\u001b[39;49mconverters,\n\u001b[0;32m    516\u001b[0m         true_values\u001b[39m=\u001b[39;49mtrue_values,\n\u001b[0;32m    517\u001b[0m         false_values\u001b[39m=\u001b[39;49mfalse_values,\n\u001b[0;32m    518\u001b[0m         skiprows\u001b[39m=\u001b[39;49mskiprows,\n\u001b[0;32m    519\u001b[0m         nrows\u001b[39m=\u001b[39;49mnrows,\n\u001b[0;32m    520\u001b[0m         na_values\u001b[39m=\u001b[39;49mna_values,\n\u001b[0;32m    521\u001b[0m         keep_default_na\u001b[39m=\u001b[39;49mkeep_default_na,\n\u001b[0;32m    522\u001b[0m         na_filter\u001b[39m=\u001b[39;49mna_filter,\n\u001b[0;32m    523\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    524\u001b[0m         parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[0;32m    525\u001b[0m         date_parser\u001b[39m=\u001b[39;49mdate_parser,\n\u001b[0;32m    526\u001b[0m         date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m    527\u001b[0m         thousands\u001b[39m=\u001b[39;49mthousands,\n\u001b[0;32m    528\u001b[0m         decimal\u001b[39m=\u001b[39;49mdecimal,\n\u001b[0;32m    529\u001b[0m         comment\u001b[39m=\u001b[39;49mcomment,\n\u001b[0;32m    530\u001b[0m         skipfooter\u001b[39m=\u001b[39;49mskipfooter,\n\u001b[0;32m    531\u001b[0m         dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[0;32m    532\u001b[0m     )\n\u001b[0;32m    533\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    534\u001b[0m     \u001b[39m# make sure to close opened file handles\u001b[39;00m\n\u001b[0;32m    535\u001b[0m     \u001b[39mif\u001b[39;00m should_close:\n",
      "File \u001b[1;32mc:\\Users\\ac252489\\wavelet_decomposition\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1616\u001b[0m, in \u001b[0;36mExcelFile.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[0;32m   1576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\n\u001b[0;32m   1577\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1578\u001b[0m     sheet_name: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mint\u001b[39m \u001b[39m|\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mint\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1596\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds,\n\u001b[0;32m   1597\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, DataFrame] \u001b[39m|\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mint\u001b[39m, DataFrame]:\n\u001b[0;32m   1598\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m \u001b[39m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1614\u001b[0m \u001b[39m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[0;32m   1615\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1616\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mparse(\n\u001b[0;32m   1617\u001b[0m         sheet_name\u001b[39m=\u001b[39;49msheet_name,\n\u001b[0;32m   1618\u001b[0m         header\u001b[39m=\u001b[39;49mheader,\n\u001b[0;32m   1619\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m   1620\u001b[0m         index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[0;32m   1621\u001b[0m         usecols\u001b[39m=\u001b[39;49musecols,\n\u001b[0;32m   1622\u001b[0m         converters\u001b[39m=\u001b[39;49mconverters,\n\u001b[0;32m   1623\u001b[0m         true_values\u001b[39m=\u001b[39;49mtrue_values,\n\u001b[0;32m   1624\u001b[0m         false_values\u001b[39m=\u001b[39;49mfalse_values,\n\u001b[0;32m   1625\u001b[0m         skiprows\u001b[39m=\u001b[39;49mskiprows,\n\u001b[0;32m   1626\u001b[0m         nrows\u001b[39m=\u001b[39;49mnrows,\n\u001b[0;32m   1627\u001b[0m         na_values\u001b[39m=\u001b[39;49mna_values,\n\u001b[0;32m   1628\u001b[0m         parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[0;32m   1629\u001b[0m         date_parser\u001b[39m=\u001b[39;49mdate_parser,\n\u001b[0;32m   1630\u001b[0m         date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   1631\u001b[0m         thousands\u001b[39m=\u001b[39;49mthousands,\n\u001b[0;32m   1632\u001b[0m         comment\u001b[39m=\u001b[39;49mcomment,\n\u001b[0;32m   1633\u001b[0m         skipfooter\u001b[39m=\u001b[39;49mskipfooter,\n\u001b[0;32m   1634\u001b[0m         dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[0;32m   1635\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds,\n\u001b[0;32m   1636\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ac252489\\wavelet_decomposition\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:916\u001b[0m, in \u001b[0;36mBaseExcelReader.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    915\u001b[0m         err\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00merr\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m (sheet: \u001b[39m\u001b[39m{\u001b[39;00masheetname\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39merr\u001b[39m.\u001b[39margs[\u001b[39m1\u001b[39m:])\n\u001b[1;32m--> 916\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    918\u001b[0m \u001b[39mif\u001b[39;00m last_sheetname \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSheet name is an empty list\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ac252489\\wavelet_decomposition\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:878\u001b[0m, in \u001b[0;36mBaseExcelReader.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[39m# GH 12292 : error when read one empty column from excel file\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 878\u001b[0m     parser \u001b[39m=\u001b[39m TextParser(\n\u001b[0;32m    879\u001b[0m         data,\n\u001b[0;32m    880\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    881\u001b[0m         header\u001b[39m=\u001b[39;49mheader,\n\u001b[0;32m    882\u001b[0m         index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[0;32m    883\u001b[0m         has_index_names\u001b[39m=\u001b[39;49mhas_index_names,\n\u001b[0;32m    884\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    885\u001b[0m         true_values\u001b[39m=\u001b[39;49mtrue_values,\n\u001b[0;32m    886\u001b[0m         false_values\u001b[39m=\u001b[39;49mfalse_values,\n\u001b[0;32m    887\u001b[0m         skiprows\u001b[39m=\u001b[39;49mskiprows,\n\u001b[0;32m    888\u001b[0m         nrows\u001b[39m=\u001b[39;49mnrows,\n\u001b[0;32m    889\u001b[0m         na_values\u001b[39m=\u001b[39;49mna_values,\n\u001b[0;32m    890\u001b[0m         skip_blank_lines\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# GH 39808\u001b[39;49;00m\n\u001b[0;32m    891\u001b[0m         parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[0;32m    892\u001b[0m         date_parser\u001b[39m=\u001b[39;49mdate_parser,\n\u001b[0;32m    893\u001b[0m         date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m    894\u001b[0m         thousands\u001b[39m=\u001b[39;49mthousands,\n\u001b[0;32m    895\u001b[0m         decimal\u001b[39m=\u001b[39;49mdecimal,\n\u001b[0;32m    896\u001b[0m         comment\u001b[39m=\u001b[39;49mcomment,\n\u001b[0;32m    897\u001b[0m         skipfooter\u001b[39m=\u001b[39;49mskipfooter,\n\u001b[0;32m    898\u001b[0m         usecols\u001b[39m=\u001b[39;49musecols,\n\u001b[0;32m    899\u001b[0m         dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[0;32m    900\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds,\n\u001b[0;32m    901\u001b[0m     )\n\u001b[0;32m    903\u001b[0m     output[asheetname] \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mread(nrows\u001b[39m=\u001b[39mnrows)\n\u001b[0;32m    905\u001b[0m     \u001b[39mif\u001b[39;00m header_names:\n",
      "File \u001b[1;32mc:\\Users\\ac252489\\wavelet_decomposition\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:2051\u001b[0m, in \u001b[0;36mTextParser\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   1998\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1999\u001b[0m \u001b[39mConverts lists of lists/tuples into DataFrames with proper type inference\u001b[39;00m\n\u001b[0;32m   2000\u001b[0m \u001b[39mand optional (e.g. string to datetime) conversion. Also enables iterating\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2048\u001b[0m \u001b[39m    `round_trip` for the round-trip converter.\u001b[39;00m\n\u001b[0;32m   2049\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2050\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39mengine\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 2051\u001b[0m \u001b[39mreturn\u001b[39;00m TextFileReader(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32mc:\\Users\\ac252489\\wavelet_decomposition\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1612\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1609\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnrows \u001b[39m=\u001b[39m options\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   1611\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_file_or_buffer(f, engine)\n\u001b[1;32m-> 1612\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_clean_options(options, engine)\n\u001b[0;32m   1614\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwds:\n\u001b[0;32m   1615\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ac252489\\wavelet_decomposition\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1794\u001b[0m, in \u001b[0;36mTextFileReader._clean_options\u001b[1;34m(self, options, engine)\u001b[0m\n\u001b[0;32m   1791\u001b[0m         index_col \u001b[39m=\u001b[39m [index_col]\n\u001b[0;32m   1792\u001b[0m result[\u001b[39m\"\u001b[39m\u001b[39mindex_col\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m index_col\n\u001b[1;32m-> 1794\u001b[0m names \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(names) \u001b[39mif\u001b[39;00m names \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m names\n\u001b[0;32m   1796\u001b[0m \u001b[39m# type conversion-related\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m \u001b[39mif\u001b[39;00m converters \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'bool' object is not iterable (sheet: 2012)"
     ]
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "# ------ Imp orting data ------------\n",
    "# ----------------------------------\n",
    "\n",
    "stacked_input_data, years = import_excel(path_input_data,input_file, \n",
    "                                    dpd ,ndpd, dpy, time_series, \n",
    "                                    interp=True) # interpolate data from dpd to ndpd numper of points per day\n",
    "\n",
    "# ------ Saving data -------\n",
    "# Once data are computed, comment lines bellow and uncomment the last one\n",
    "pkl.dump([stacked_input_data, years], open('imported_signal' + '.p', \"wb\") )\n",
    "\n",
    "# Uncomment once date are computed\n",
    "# [stacked_input_data, years]= pkl.load(open('imported_signal' + '.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting the wavelet decomposition\n",
    "\n",
    "<br />\n",
    "\n",
    "Signal is decomposed over a set of wavelets of 15 time scales :  [0.75, 1.5, 3., 6., 12, 24., 42., 84., 168., 273.75, 547.5, 1095., 2190., 4380.,\n",
    "              8760.] hours\n",
    "              \n",
    "The time scale could be understood as the duration of charge-discharge of a storage device\n",
    "\n",
    "<br />\n",
    "\n",
    "<ins>This set of wavelets is built such that:</ins>\n",
    "\n",
    "\n",
    "\n",
    "- There is 3 mother wavelets with different durations : _Year_ , _Week_ and _Day_\n",
    "<br />\n",
    "\n",
    "- Those 3 wavelets have a physical meaning and account for human and season cycles. Therefore, we want to make sure that the year time scale is properly center over the year (summer un summer and winter in winter), that dayly wavelets are starting from 12pm to 12 pm and not 5am to 5am,...\n",
    "<br />\n",
    "\n",
    "- For this  reason we should compute and use translations, as shown in the cell below\n",
    "<br />\n",
    "\n",
    "- The mother wavelets are then divided in daugther wavelets\n",
    "\n",
    "<br />\n",
    "\n",
    "_The 3 mother wavelets have to be adjusted on the signal: the _year_ should fit on the seasonal fluctuation, the _week_ on the week and weekend cycle, the _day_ on the nigh / day cycle\n",
    "    * Translations are computed with the \"Consumption signal\" for each year\n",
    "    * The same translations are reused for for the other time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# ------ Translations ------------\n",
    "# ----------------------------------\n",
    "'''\n",
    "First, optimal translations for the year, week and day mother wavelets\n",
    "- if do_calc is false, translations are imported.\n",
    "'''\n",
    "trans_file = 'trans_square_12_18'\n",
    "\n",
    "trans_square = load_trans(path_trans, trans_file,\n",
    "                   stacked_input_data, 'Consumption',\n",
    "                   ndpd, dpy, \n",
    "                   'square', #shape of the wavelet: either 'square' or 'sine'\n",
    "                    do_calc = False) # False: import translations. True: compute new translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_function= [2]*365*8+ [0]*365*8 + [1]*365*(64-16)\n",
    "len(test_function)== 365*64\n",
    "plt.plot(test_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked_input_data = {'test': test_function}\n",
    "# time_series = ['test']\n",
    "# signal = 'test'\n",
    "# years=['2012']\n",
    "# year = '2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# ------ Performing wavelet decomposition ------------\n",
    "# ----------------------------------------------------\n",
    "'''\n",
    "Second, we compute the coefficients of the wavelet decomposition\n",
    "- wl_shape : either 'sine' or 'square' shape\n",
    "- To compute new matrix use imp_matrix = False. It can take quite some time !\n",
    "'''\n",
    "path_matrix = \"saved_matrix\\\\square_shape\\\\\"\n",
    "stacked_betas_square, saved_sheets_square = compute_betas(time_series, stacked_input_data,\n",
    "                 vy, vw, vd, dpy, ndpd, years,\n",
    "                 trans_square,\n",
    "                 path_matrix,\n",
    "                 beta_path, wl_shape ='square', imp_matrix = True)\n",
    "\n",
    "pkl.dump([stacked_betas_square, saved_sheets_square], open('betas_saved_square' + '.p', \"wb\"))\n",
    "\n",
    "# [stacked_betas_square, saved_sheets_square] = pkl.load(open('betas_saved_square' + '.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = preplotprocessing(vy, vw , vd, ndpd, dpy, signal, #Then the kind of signal. In this tutorial its either 'Consommation', 'Eolien' or 'Solaire\n",
    "#                       year, years,\n",
    "#                       saved_sheets_square, # Here is the datasheet you want to plot. This datasheet is returned by the function compute_betas(). Here is ploted the square decomposition\n",
    "#                       do_trans = None) # Eventualy retranslate the decomposition to make it feet to the week, day and sesonal cycles\n",
    "# plot_betas_heatmap(df, signal, year , ndpd,\n",
    "#                       cmin= -0.1,\n",
    "#                       cmax= 0.1,\n",
    "#                     ccenter = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The wavelet decomposition is saved in a disctionnary with the following sstructure:\n",
    "print(saved_sheets_square.keys())\n",
    "print(saved_sheets_square['Consumption'].keys())\n",
    "print(saved_sheets_square['Consumption'][\"2012\"][14]) # 14th time scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps\n",
    "\n",
    "<br />\n",
    "\n",
    "The wavelet decomposition can be depicted with a heatmaps.\n",
    "\n",
    "<br />\n",
    "\n",
    "Example in the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, select the year\n",
    "year = '2018'\n",
    "# The directory where the matrix is saved. In this case, we are using square matric\n",
    "# Replace square by sine for the sine wavelet decomposition\n",
    "path_matrix = \"saved_matrix\\\\square_shape\\\\\"\n",
    "matrix_name = 'A_'+ year+'.npz'\n",
    "\n",
    "A_sparse  = sparse.load_npz(path_matrix + matrix_name)\n",
    "A = sparse.csr_matrix.todense(A_sparse)\n",
    "matrix = np.asarray(A)\n",
    "# The signal on which you want to process the wavelet decomposition : 'Consumption', 'Wind' or 'PV'\n",
    "signal = 'Consumption'\n",
    "# \n",
    "df= preplotprocessing(vy, vw , vd, ndpd, dpy, signal, #Then the kind of signal. In this tutorial its either 'Consommation', 'Eolien' or 'Solaire\n",
    "                      year, years,\n",
    "                      time_scales,\n",
    "                      saved_sheets_square, # Here is the datasheet you want to plot. This datasheet is returned by the function compute_betas(). Here is ploted the square decomposition\n",
    "                      matrix)\n",
    "\n",
    "plot_betas_heatmap(df, signal, year , ndpd,\n",
    "                      cmin= -0.1,\n",
    "                      cmax= 0.1,\n",
    "                    ccenter = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wavelet _Versus_ Fourier transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year='2018' # year of the decomposition\n",
    "index = years.index(year)\n",
    "\n",
    "signal = 'Consumption'\n",
    "\n",
    "input_data = stacked_input_data[signal][signal_length*index:signal_length*(index+1)]\n",
    "fft(ndpd, dpy, signal, year, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Energy, Power, and Nb of Cycles\n",
    "<br/>\n",
    "\n",
    "### From the coefficients betas of the wavelet decomposition, we can determine the flexibility required by the energy system\n",
    "\n",
    "<br/>\n",
    "\n",
    "***Satisfaction rate:*** Percentage of the time the consumption is satisfied. Examples:\n",
    "* If satisfaction rate = 90%: At the day scales 0.9*365=328 days per year, the energy demand will be satisfied. We get rid of the 365-328=37 days with the highest electricity consumption\n",
    "* Here the signal last 7 years. With a 90% satisfaction rate we get rid of the 37*7 extreme days over the last 7 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example for an extreme case : 100% of electricity is produced by PV\n",
    "scenario_name = 'PV'\n",
    "conso = stacked_betas_square['Consumption']\n",
    "prod =  stacked_betas_square['PV']\n",
    "\n",
    "pmc = [np.array(prod[i]) - np.array(conso[i]) for i in range(len(time_scales)) ]\n",
    "\n",
    "satisfactions = [80,95,99,100] # Satisfaction rate\n",
    "# Percentage of the time the load will be met by the storage device.\n",
    "\n",
    "satisfaction_labels = [str(satis)+'%' for satis in satisfactions]\n",
    "# -------------------------------\n",
    "# ------- Compute and plot E,P and N\n",
    "#\n",
    "results = calc_epn(pmc, satisfactions, time_scales, dpy, load_factor, shape = 'square')\n",
    "plot_EPN(results['emax'], results['pmax'], results['n'], results['uf'], results['serv'],\n",
    "         time_scales, satisfactions, scenario_name )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstructing and filtering signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Importing Matrix for the reconstruction\n",
    "year = '2013'\n",
    "\n",
    "path_matrix = \"saved_matrix\\\\square_shape\\\\\"\n",
    "# path_matrix = \"saved_matrix\\\\sine_shape\\\\\"\n",
    "matrix_name = 'A_'+ year+'.npz'\n",
    "\n",
    "A_sparse  = sparse.load_npz(path_matrix + matrix_name)\n",
    "A = sparse.csr_matrix.todense(A_sparse)\n",
    "matrix = np.asarray(A)\n",
    "# #\n",
    "reconstruct(time_scales, [12, 24],\n",
    "                matrix,saved_sheets_square['PV'][year], \"2013 PV signal filtered with 12h and 24h square wavelets\",\n",
    "                xmin=150, xmax=154,\n",
    "                dpy=365, dpd=64,\n",
    "                add_offset=True)\n",
    "\n",
    "reconstruct(time_scales, [24],\n",
    "                matrix,saved_sheets_square['PV'][year], \"2013 PV signal filtered with 24h square wavelets\",\n",
    "                xmin=150, xmax=154,\n",
    "                dpy=365, dpd=64,\n",
    "                add_offset=True)\n",
    "\n",
    "reconstruct(time_scales, time_scales,\n",
    "                matrix,saved_sheets_square['PV'][year], \"Reconstructed PV signal\",\n",
    "                xmin=0, xmax=365,\n",
    "                dpy=365, dpd=64,\n",
    "                add_offset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To translate or not translate ?\n",
    "<br/>\n",
    "<center> <bold> Results comparison with translations and without translations </bold></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# ------ Create translations full of zeros -----\n",
    "# -----------------------------------------------\n",
    "zero_trans = []\n",
    "Nyears = int(len(stacked_input_data['Consumption'])/(ndpd*dpy) )\n",
    "for i in range(Nyears):\n",
    "    zero_trans.append([0,0,0])\n",
    "    \n",
    "# ----------------------------------------------------\n",
    "# ------ Decomposition with without translations -----\n",
    "# ----------------------------------------------------\n",
    "\n",
    "path_matrix = \"saved_matrix\\\\square_shape\\\\no_trans\\\\\"\n",
    "beta_path = \"betas\\\\square_shape_no_trans\\\\\"\n",
    "\n",
    "stacked_betas_square_no_trans, saved_sheets_square_no_trans = compute_betas(time_series, stacked_input_data,\n",
    "                 vy, vw, vd, dpy, ndpd, years,\n",
    "                 zero_trans,\n",
    "                 path_matrix,\n",
    "                 beta_path, wl_shape ='square', imp_matrix = True)\n",
    "\n",
    "pkl.dump([stacked_betas_square_no_trans, saved_sheets_square_no_trans], open('betas_saved_square_no_trans' + '.p', \"wb\"))\n",
    "\n",
    "# [stacked_betas_square_no_trans, saved_sheets_square_no_trans] = pkl.load(open('betas_saved_square_no_trans' + '.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# -- Same plots without translations ---\n",
    "# --------------------------------------\n",
    "scenario_name = 'PV, square wavelet decomposition without translation'\n",
    "conso = stacked_betas_square_no_trans['Consumption']\n",
    "prod =  stacked_betas_square_no_trans['PV']\n",
    "\n",
    "pmc = [np.array(prod[i]) - np.array(conso[i]) for i in range(len(time_scales)) ]\n",
    "\n",
    "satisfactions = [80,95,99,100]\n",
    "satisfaction_labels = [str(satis)+'%' for satis in satisfactions]\n",
    "# -------------------------------\n",
    "# ------- Compute and plot E,P and N\n",
    "#\n",
    "results_square_PV_no_trans = calc_epn(pmc, satisfactions, time_scales, dpy, load_factor, shape = 'square')\n",
    "plot_EPN(results_square_PV_no_trans['emax'], results_square_PV_no_trans['pmax'], results_square_PV_no_trans['n'], results_square_PV_no_trans['uf'], results_square_PV_no_trans['serv'],\n",
    "         time_scales, satisfactions, scenario_name )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sine and square shape wavelets\n",
    "<br/>\n",
    "Wavelets can take various shapes. We used here both sine and square shape. Here is and example of a sine shape decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Import translations -----\n",
    "trans_file = 'trans_sine_12_18'\n",
    "\n",
    "trans_sine = load_trans(path_trans, trans_file,\n",
    "                   stacked_input_data, 'Consommation',\n",
    "                   ndpd, dpy, \n",
    "                   'sine', do_calc=False)\n",
    "# ------Perform wavelet decomposition ----\n",
    "\n",
    "beta_path = \"betas\\\\sine_shape\\\\\"\n",
    "\n",
    "path_matrix = \"saved_matrix\\\\sine_shape\\\\\"\n",
    "\n",
    "stacked_betas_sine, saved_sheets_sine = compute_betas(time_series, stacked_input_data,\n",
    "                 vy,vw,vd,dpy, ndpd, years,\n",
    "                 trans_sine,\n",
    "                 path_matrix,\n",
    "                 beta_path, wl_shape ='sine', imp_matrix = True)\n",
    "\n",
    "# ----- Plot 100% PV scenario ---\n",
    "\n",
    "scenario_name = 'PV, sine wavelet decomposition'\n",
    "sine_conso = stacked_betas_sine['Consumption']\n",
    "sine_prod =  stacked_betas_sine['PV']\n",
    "\n",
    "sine_pmc = [np.array(sine_prod[i]) - np.array(sine_conso[i]) for i in range(len(time_scales)) ]\n",
    "\n",
    "satisfactions = [80,95,99,100]\n",
    "satisfaction_labels = [str(satis)+'%' for satis in satisfactions]\n",
    "# -------------------------------\n",
    "# ------- Compute and plot E,P and N\n",
    "#\n",
    "results_sine = calc_epn(sine_pmc, satisfactions, time_scales, dpy, load_factor, shape = 'sine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_EPN(results_sine['emax'], results_sine['pmax'], results_sine['n'], results_sine['uf'], results_sine['serv'],\n",
    "         time_scales, satisfactions, scenario_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Reconstruct signal -----\n",
    "\n",
    "year = '2013'\n",
    "path_matrix = \"saved_matrix\\\\sine_shape\\\\\"\n",
    "matrix_name = 'A_'+ year+'.npz'\n",
    "\n",
    "A_sparse  = sparse.load_npz(path_matrix + matrix_name)\n",
    "A = sparse.csr_matrix.todense(A_sparse)\n",
    "matrix = np.asarray(A)\n",
    "# #\n",
    "reconstruct(time_scales, [24],\n",
    "                matrix,saved_sheets_sine['PV'][year], \"Signal PV 2013 with 24 hours sine wavelets\",\n",
    "                xmin=0, xmax=15,\n",
    "                dpy=365, dpd=64,\n",
    "                add_offset=True)\n",
    "\n",
    "reconstruct(time_scales, time_scales,\n",
    "                matrix,saved_sheets_sine['PV'][year], \"2013 PV signal reconsruction\",\n",
    "                xmin=0, xmax=365,\n",
    "                dpy=365, dpd=64,\n",
    "                add_offset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
